{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":824162,"sourceType":"datasetVersion","datasetId":433728}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T11:41:52.206877Z","iopub.execute_input":"2024-04-27T11:41:52.207394Z","iopub.status.idle":"2024-04-27T11:41:52.212251Z","shell.execute_reply.started":"2024-04-27T11:41:52.207355Z","shell.execute_reply":"2024-04-27T11:41:52.211384Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"* Dataset consists of keystroke-timing information from 51 subjects.\n* Each subject typed a given password 400 times.\n* 34 timing features per sample, capturing hold times and key-down to key-up dynamics.\n* Unique identifiers used for each subject to ensure anonymity and security.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/keystroke-dynamics-benchmark-data-set/DSL-StrongPasswordData.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:42:11.348119Z","iopub.execute_input":"2024-04-27T11:42:11.348518Z","iopub.status.idle":"2024-04-27T11:42:11.528864Z","shell.execute_reply.started":"2024-04-27T11:42:11.348488Z","shell.execute_reply":"2024-04-27T11:42:11.527448Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"      subject  sessionIndex  rep  H.period  DD.period.t  UD.period.t     H.t  \\\n0        s002             1    1    0.1491       0.3979       0.2488  0.1069   \n1        s002             1    2    0.1111       0.3451       0.2340  0.0694   \n2        s002             1    3    0.1328       0.2072       0.0744  0.0731   \n3        s002             1    4    0.1291       0.2515       0.1224  0.1059   \n4        s002             1    5    0.1249       0.2317       0.1068  0.0895   \n...       ...           ...  ...       ...          ...          ...     ...   \n20395    s057             8   46    0.0884       0.0685      -0.0199  0.1095   \n20396    s057             8   47    0.0655       0.0630      -0.0025  0.0910   \n20397    s057             8   48    0.0939       0.1189       0.0250  0.1008   \n20398    s057             8   49    0.0923       0.1294       0.0371  0.0913   \n20399    s057             8   50    0.0596       0.1310       0.0714  0.0992   \n\n       DD.t.i  UD.t.i     H.i  ...     H.a  DD.a.n  UD.a.n     H.n  DD.n.l  \\\n0      0.1674  0.0605  0.1169  ...  0.1349  0.1484  0.0135  0.0932  0.3515   \n1      0.1283  0.0589  0.0908  ...  0.1412  0.2558  0.1146  0.1146  0.2642   \n2      0.1291  0.0560  0.0821  ...  0.1621  0.2332  0.0711  0.1172  0.2705   \n3      0.2495  0.1436  0.1040  ...  0.1457  0.1629  0.0172  0.0866  0.2341   \n4      0.1676  0.0781  0.0903  ...  0.1312  0.1582  0.0270  0.0884  0.2517   \n...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n20395  0.1290  0.0195  0.0945  ...  0.1219  0.1383  0.0164  0.0820  0.1329   \n20396  0.1148  0.0238  0.0916  ...  0.1008  0.0512 -0.0496  0.1037  0.0868   \n20397  0.1122  0.0114  0.0721  ...  0.0913  0.1169  0.0256  0.0689  0.1311   \n20398  0.0990  0.0077  0.0992  ...  0.0882  0.0821 -0.0061  0.0576  0.0697   \n20399  0.1103  0.0111  0.0998  ...  0.0969  0.0784 -0.0185  0.0790  0.1133   \n\n       UD.n.l     H.l  DD.l.Return  UD.l.Return  H.Return  \n0      0.2583  0.1338       0.3509       0.2171    0.0742  \n1      0.1496  0.0839       0.2756       0.1917    0.0747  \n2      0.1533  0.1085       0.2847       0.1762    0.0945  \n3      0.1475  0.0845       0.3232       0.2387    0.0813  \n4      0.1633  0.0903       0.2517       0.1614    0.0818  \n...       ...     ...          ...          ...       ...  \n20395  0.0509  0.1005       0.2054       0.1049    0.1047  \n20396 -0.0169  0.1445       0.2206       0.0761    0.1198  \n20397  0.0622  0.1034       0.2017       0.0983    0.0905  \n20398  0.0121  0.0979       0.1917       0.0938    0.0931  \n20399  0.0343  0.0807       0.1993       0.1186    0.1018  \n\n[20400 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>sessionIndex</th>\n      <th>rep</th>\n      <th>H.period</th>\n      <th>DD.period.t</th>\n      <th>UD.period.t</th>\n      <th>H.t</th>\n      <th>DD.t.i</th>\n      <th>UD.t.i</th>\n      <th>H.i</th>\n      <th>...</th>\n      <th>H.a</th>\n      <th>DD.a.n</th>\n      <th>UD.a.n</th>\n      <th>H.n</th>\n      <th>DD.n.l</th>\n      <th>UD.n.l</th>\n      <th>H.l</th>\n      <th>DD.l.Return</th>\n      <th>UD.l.Return</th>\n      <th>H.Return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>s002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.1491</td>\n      <td>0.3979</td>\n      <td>0.2488</td>\n      <td>0.1069</td>\n      <td>0.1674</td>\n      <td>0.0605</td>\n      <td>0.1169</td>\n      <td>...</td>\n      <td>0.1349</td>\n      <td>0.1484</td>\n      <td>0.0135</td>\n      <td>0.0932</td>\n      <td>0.3515</td>\n      <td>0.2583</td>\n      <td>0.1338</td>\n      <td>0.3509</td>\n      <td>0.2171</td>\n      <td>0.0742</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>s002</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.1111</td>\n      <td>0.3451</td>\n      <td>0.2340</td>\n      <td>0.0694</td>\n      <td>0.1283</td>\n      <td>0.0589</td>\n      <td>0.0908</td>\n      <td>...</td>\n      <td>0.1412</td>\n      <td>0.2558</td>\n      <td>0.1146</td>\n      <td>0.1146</td>\n      <td>0.2642</td>\n      <td>0.1496</td>\n      <td>0.0839</td>\n      <td>0.2756</td>\n      <td>0.1917</td>\n      <td>0.0747</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>s002</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.1328</td>\n      <td>0.2072</td>\n      <td>0.0744</td>\n      <td>0.0731</td>\n      <td>0.1291</td>\n      <td>0.0560</td>\n      <td>0.0821</td>\n      <td>...</td>\n      <td>0.1621</td>\n      <td>0.2332</td>\n      <td>0.0711</td>\n      <td>0.1172</td>\n      <td>0.2705</td>\n      <td>0.1533</td>\n      <td>0.1085</td>\n      <td>0.2847</td>\n      <td>0.1762</td>\n      <td>0.0945</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>s002</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.1291</td>\n      <td>0.2515</td>\n      <td>0.1224</td>\n      <td>0.1059</td>\n      <td>0.2495</td>\n      <td>0.1436</td>\n      <td>0.1040</td>\n      <td>...</td>\n      <td>0.1457</td>\n      <td>0.1629</td>\n      <td>0.0172</td>\n      <td>0.0866</td>\n      <td>0.2341</td>\n      <td>0.1475</td>\n      <td>0.0845</td>\n      <td>0.3232</td>\n      <td>0.2387</td>\n      <td>0.0813</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>s002</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0.1249</td>\n      <td>0.2317</td>\n      <td>0.1068</td>\n      <td>0.0895</td>\n      <td>0.1676</td>\n      <td>0.0781</td>\n      <td>0.0903</td>\n      <td>...</td>\n      <td>0.1312</td>\n      <td>0.1582</td>\n      <td>0.0270</td>\n      <td>0.0884</td>\n      <td>0.2517</td>\n      <td>0.1633</td>\n      <td>0.0903</td>\n      <td>0.2517</td>\n      <td>0.1614</td>\n      <td>0.0818</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20395</th>\n      <td>s057</td>\n      <td>8</td>\n      <td>46</td>\n      <td>0.0884</td>\n      <td>0.0685</td>\n      <td>-0.0199</td>\n      <td>0.1095</td>\n      <td>0.1290</td>\n      <td>0.0195</td>\n      <td>0.0945</td>\n      <td>...</td>\n      <td>0.1219</td>\n      <td>0.1383</td>\n      <td>0.0164</td>\n      <td>0.0820</td>\n      <td>0.1329</td>\n      <td>0.0509</td>\n      <td>0.1005</td>\n      <td>0.2054</td>\n      <td>0.1049</td>\n      <td>0.1047</td>\n    </tr>\n    <tr>\n      <th>20396</th>\n      <td>s057</td>\n      <td>8</td>\n      <td>47</td>\n      <td>0.0655</td>\n      <td>0.0630</td>\n      <td>-0.0025</td>\n      <td>0.0910</td>\n      <td>0.1148</td>\n      <td>0.0238</td>\n      <td>0.0916</td>\n      <td>...</td>\n      <td>0.1008</td>\n      <td>0.0512</td>\n      <td>-0.0496</td>\n      <td>0.1037</td>\n      <td>0.0868</td>\n      <td>-0.0169</td>\n      <td>0.1445</td>\n      <td>0.2206</td>\n      <td>0.0761</td>\n      <td>0.1198</td>\n    </tr>\n    <tr>\n      <th>20397</th>\n      <td>s057</td>\n      <td>8</td>\n      <td>48</td>\n      <td>0.0939</td>\n      <td>0.1189</td>\n      <td>0.0250</td>\n      <td>0.1008</td>\n      <td>0.1122</td>\n      <td>0.0114</td>\n      <td>0.0721</td>\n      <td>...</td>\n      <td>0.0913</td>\n      <td>0.1169</td>\n      <td>0.0256</td>\n      <td>0.0689</td>\n      <td>0.1311</td>\n      <td>0.0622</td>\n      <td>0.1034</td>\n      <td>0.2017</td>\n      <td>0.0983</td>\n      <td>0.0905</td>\n    </tr>\n    <tr>\n      <th>20398</th>\n      <td>s057</td>\n      <td>8</td>\n      <td>49</td>\n      <td>0.0923</td>\n      <td>0.1294</td>\n      <td>0.0371</td>\n      <td>0.0913</td>\n      <td>0.0990</td>\n      <td>0.0077</td>\n      <td>0.0992</td>\n      <td>...</td>\n      <td>0.0882</td>\n      <td>0.0821</td>\n      <td>-0.0061</td>\n      <td>0.0576</td>\n      <td>0.0697</td>\n      <td>0.0121</td>\n      <td>0.0979</td>\n      <td>0.1917</td>\n      <td>0.0938</td>\n      <td>0.0931</td>\n    </tr>\n    <tr>\n      <th>20399</th>\n      <td>s057</td>\n      <td>8</td>\n      <td>50</td>\n      <td>0.0596</td>\n      <td>0.1310</td>\n      <td>0.0714</td>\n      <td>0.0992</td>\n      <td>0.1103</td>\n      <td>0.0111</td>\n      <td>0.0998</td>\n      <td>...</td>\n      <td>0.0969</td>\n      <td>0.0784</td>\n      <td>-0.0185</td>\n      <td>0.0790</td>\n      <td>0.1133</td>\n      <td>0.0343</td>\n      <td>0.0807</td>\n      <td>0.1993</td>\n      <td>0.1186</td>\n      <td>0.1018</td>\n    </tr>\n  </tbody>\n</table>\n<p>20400 rows × 34 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:42:13.693486Z","iopub.execute_input":"2024-04-27T11:42:13.695248Z","iopub.status.idle":"2024-04-27T11:42:13.713359Z","shell.execute_reply.started":"2024-04-27T11:42:13.695147Z","shell.execute_reply":"2024-04-27T11:42:13.712292Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20400 entries, 0 to 20399\nData columns (total 34 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   subject          20400 non-null  object \n 1   sessionIndex     20400 non-null  int64  \n 2   rep              20400 non-null  int64  \n 3   H.period         20400 non-null  float64\n 4   DD.period.t      20400 non-null  float64\n 5   UD.period.t      20400 non-null  float64\n 6   H.t              20400 non-null  float64\n 7   DD.t.i           20400 non-null  float64\n 8   UD.t.i           20400 non-null  float64\n 9   H.i              20400 non-null  float64\n 10  DD.i.e           20400 non-null  float64\n 11  UD.i.e           20400 non-null  float64\n 12  H.e              20400 non-null  float64\n 13  DD.e.five        20400 non-null  float64\n 14  UD.e.five        20400 non-null  float64\n 15  H.five           20400 non-null  float64\n 16  DD.five.Shift.r  20400 non-null  float64\n 17  UD.five.Shift.r  20400 non-null  float64\n 18  H.Shift.r        20400 non-null  float64\n 19  DD.Shift.r.o     20400 non-null  float64\n 20  UD.Shift.r.o     20400 non-null  float64\n 21  H.o              20400 non-null  float64\n 22  DD.o.a           20400 non-null  float64\n 23  UD.o.a           20400 non-null  float64\n 24  H.a              20400 non-null  float64\n 25  DD.a.n           20400 non-null  float64\n 26  UD.a.n           20400 non-null  float64\n 27  H.n              20400 non-null  float64\n 28  DD.n.l           20400 non-null  float64\n 29  UD.n.l           20400 non-null  float64\n 30  H.l              20400 non-null  float64\n 31  DD.l.Return      20400 non-null  float64\n 32  UD.l.Return      20400 non-null  float64\n 33  H.Return         20400 non-null  float64\ndtypes: float64(31), int64(2), object(1)\nmemory usage: 5.3+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.drop(columns=['sessionIndex', 'rep'], axis=1, inplace=True)\n\nle = LabelEncoder()\ndf['subject'] = le.fit_transform(df['subject'])\n\n# Extract features and labels\nX = df.iloc[:, 1:].values\ny = df.iloc[:, 0].values\n\n# One hot encoding of the labels\ny = to_categorical(y)\n\n\n# Normalization\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = X.reshape(X.shape[0], 1, X.shape[1])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:42:18.005576Z","iopub.execute_input":"2024-04-27T11:42:18.005948Z","iopub.status.idle":"2024-04-27T11:42:18.048548Z","shell.execute_reply.started":"2024-04-27T11:42:18.005919Z","shell.execute_reply":"2024-04-27T11:42:18.047038Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten, Dropout\n\nnum_classes = y.shape[1]\n\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(1, X.shape[2]), padding='same'))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:45:00.045296Z","iopub.execute_input":"2024-04-27T11:45:00.045692Z","iopub.status.idle":"2024-04-27T11:45:14.154873Z","shell.execute_reply.started":"2024-04-27T11:45:00.045661Z","shell.execute_reply":"2024-04-27T11:45:14.153583Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m6,016\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m6,500\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m)             │         \u001b[38;5;34m5,151\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,016</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,151</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,667\u001b[0m (69.01 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,667</span> (69.01 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,667\u001b[0m (69.01 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,667</span> (69.01 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.1520 - loss: 3.4077 - val_accuracy: 0.6375 - val_loss: 1.6378\nEpoch 2/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5196 - loss: 1.7447 - val_accuracy: 0.7657 - val_loss: 0.9903\nEpoch 3/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6462 - loss: 1.2606 - val_accuracy: 0.7961 - val_loss: 0.7988\nEpoch 4/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7115 - loss: 1.0375 - val_accuracy: 0.8211 - val_loss: 0.6803\nEpoch 5/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7309 - loss: 0.9436 - val_accuracy: 0.8363 - val_loss: 0.6087\nEpoch 6/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7598 - loss: 0.8624 - val_accuracy: 0.8493 - val_loss: 0.5644\nEpoch 7/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7694 - loss: 0.8151 - val_accuracy: 0.8554 - val_loss: 0.5203\nEpoch 8/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7859 - loss: 0.7496 - val_accuracy: 0.8657 - val_loss: 0.4851\nEpoch 9/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7974 - loss: 0.7040 - val_accuracy: 0.8667 - val_loss: 0.4741\nEpoch 10/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8131 - loss: 0.6573 - val_accuracy: 0.8716 - val_loss: 0.4505\nEpoch 11/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8153 - loss: 0.6457 - val_accuracy: 0.8801 - val_loss: 0.4252\nEpoch 12/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8209 - loss: 0.6217 - val_accuracy: 0.8850 - val_loss: 0.4088\nEpoch 13/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8275 - loss: 0.5841 - val_accuracy: 0.8865 - val_loss: 0.3990\nEpoch 14/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8375 - loss: 0.5666 - val_accuracy: 0.8873 - val_loss: 0.3886\nEpoch 15/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8373 - loss: 0.5553 - val_accuracy: 0.8887 - val_loss: 0.3788\nEpoch 16/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8349 - loss: 0.5516 - val_accuracy: 0.8912 - val_loss: 0.3659\nEpoch 17/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8473 - loss: 0.5175 - val_accuracy: 0.8946 - val_loss: 0.3595\nEpoch 18/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8471 - loss: 0.5078 - val_accuracy: 0.8924 - val_loss: 0.3560\nEpoch 19/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8470 - loss: 0.5071 - val_accuracy: 0.8971 - val_loss: 0.3465\nEpoch 20/20\n\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8493 - loss: 0.4967 - val_accuracy: 0.8983 - val_loss: 0.3413\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7c338c411870>"},"metadata":{}}]},{"cell_type":"code","source":"# Model Evaluation\nscores = model.evaluate(X_test, y_test)\nprint(f\"Accuracy: {scores[1]*100}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:45:19.195857Z","iopub.execute_input":"2024-04-27T11:45:19.196242Z","iopub.status.idle":"2024-04-27T11:45:19.429775Z","shell.execute_reply.started":"2024-04-27T11:45:19.196213Z","shell.execute_reply":"2024-04-27T11:45:19.428620Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9043 - loss: 0.3284\nAccuracy: 89.82843160629272%\n","output_type":"stream"}]}]}